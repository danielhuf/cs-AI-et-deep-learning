{"cells":[{"cell_type":"markdown","metadata":{"id":"PUu6giBbG6Ol"},"source":["This notebook is adapted from [this one](https://github.com/dataflowr/notebooks/blob/archive-2020/Notebooks/02_basics_pytorch.ipynb) written by [Marc Lelarge](https://www.di.ens.fr/~lelarge/), and from the tutorials [What is PyTorch?](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#) and [Autograd: Automatic Differentiation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py).\n","\n","You can have access to more tutorials [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n","\n","When you have questions, try as much as possible to find the answer by looking at [PyTorch documentation](https://pytorch.org/docs/stable/index.html). You need to learn to work with the documentation."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1674255857815,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"hiN-IpzEG6Oo"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"AY6hOsYQG6Oq"},"source":["\n","# What is PyTorch?\n","\n","It’s a Python-based scientific computing package targeted at two sets of\n","usage:\n","\n","-  A replacement for NumPy to use the power of GPUs\n","-  A deep learning research platform that provides maximum flexibility\n","   and speed\n","\n","## Tensors\n","\n","Tensors are similar to NumPy’s ndarrays, with the addition being that\n","Tensors can also be used on a GPU to accelerate computing.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"I_ib-7YOG6Oz"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","metadata":{"id":"zpoJFuPhG6Oz"},"source":["Construct a 5x3 matrix, uninitialized:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ihnJhdT0G6O0"},"source":["\u003cdiv class=\"alert alert-info\"\u003e\u003ch4\u003eNote\u003c/h4\u003e\u003cp\u003eAn uninitialized matrix is declared,\n","    but does not contain definite known\n","    values before it is used. When an\n","    uninitialized matrix is created,\n","    whatever values were in the allocated\n","    memory at the time will appear as the initial values.\u003c/p\u003e\u003c/div\u003e\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1670408755605,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"mB_RbhrBG6O0","outputId":"55de8242-10eb-47e6-928e-44fbda175389"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1.0377e-35, 0.0000e+00, 3.3631e-44],\n","        [0.0000e+00,        nan, 0.0000e+00],\n","        [1.1578e+27, 1.1362e+30, 7.1547e+22],\n","        [4.5828e+30, 1.2121e+04, 7.1846e+22],\n","        [9.2198e-39, 7.0374e+22, 0.0000e+00]])\n"]}],"source":["x = torch.empty(5, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"kmSaiWC4G6O1"},"source":["Construct a randomly initialized matrix:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1670408768429,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"fGiQI_8PG6O1","outputId":"ba9d4839-6978-4ceb-e0dc-41110f5b1097"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.3389, 0.6217, 0.7961],\n","        [0.8274, 0.1468, 0.5900],\n","        [0.2381, 0.8871, 0.7395],\n","        [0.7638, 0.7922, 0.5789],\n","        [0.4839, 0.3332, 0.0893]])\n"]}],"source":["x = torch.rand(5, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"UzaZGJgnG6O1"},"source":["Construct a matrix filled zeros and of dtype long:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1670408780149,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"YEnLrMBTG6O2","outputId":"dbf71079-a64d-4ded-cbc6-7224eef2f53a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]])\n"]}],"source":["x = torch.zeros(5, 3, dtype=torch.long)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"HWc7E1nEG6O2"},"source":["Construct a tensor directly from data:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1670408803422,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"RDeHzBshG6O3","outputId":"8c9ce87a-45e6-4c2a-9f2a-c882ce7454d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5.5000, 3.0000])\n"]}],"source":["x = torch.tensor([5.5, 3])\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"ZX6W35ygG6O3"},"source":["or create a tensor based on an existing tensor. These methods\n","will reuse properties of the input tensor, e.g. dtype, unless\n","new values are provided by user\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259,"status":"ok","timestamp":1670408832270,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"3uKBepkHG6O3","outputId":"6336de44-9ea4-4030-ded0-774ca1dbd24e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]], dtype=torch.float64)\n","tensor([[ 1.0252,  0.2231, -0.6015],\n","        [-0.9343,  2.2719, -1.2785],\n","        [ 0.1208, -1.7435,  0.8674],\n","        [-1.4260, -0.0106,  0.8932],\n","        [-0.4768,  1.1218,  0.0462]])\n"]}],"source":["x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n","print(x)\n","\n","x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n","print(x)                                      # result has the same size"]},{"cell_type":"markdown","metadata":{"id":"1owyKwIqG6O4"},"source":["Get its size:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1670408852332,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"ACWmYhPBG6O4","outputId":"fd36c203-f2d0-400c-e121-3185a814bf82"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 3])\n"]}],"source":["print(x.size())"]},{"cell_type":"markdown","metadata":{"id":"0-MzkfnSG6O5"},"source":["\u003cdiv class=\"alert alert-info\"\u003e\u003ch4\u003eNote\u003c/h4\u003e\u003cp\u003e``torch.Size`` is in fact a tuple, so it supports all tuple operations.\u003c/p\u003e\u003c/div\u003e\n","\n","## Operations\n","\n","There are multiple syntaxes for operations. In the following\n","example, we will take a look at the addition operation.\n","\n","Addition: syntax 1\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214,"status":"ok","timestamp":1670408898344,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"31Sq8nl1G6O5","outputId":"62a2ca61-4d7d-49fb-f6b1-942488859f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 1.4519,  0.7168, -0.4850],\n","        [-0.8998,  2.9129, -0.6419],\n","        [ 1.0427, -1.3704,  1.3449],\n","        [-1.1606,  0.0387,  1.4207],\n","        [-0.2107,  1.4956,  0.9662]])\n"]}],"source":["y = torch.rand(5, 3)\n","print(x + y)"]},{"cell_type":"markdown","metadata":{"id":"Az-7LxDSG6O6"},"source":["Addition: syntax 2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1670408904720,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"Y1NEDTyyG6O6","outputId":"e118dd08-048d-46e5-87ed-f50c528c06d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 1.4519,  0.7168, -0.4850],\n","        [-0.8998,  2.9129, -0.6419],\n","        [ 1.0427, -1.3704,  1.3449],\n","        [-1.1606,  0.0387,  1.4207],\n","        [-0.2107,  1.4956,  0.9662]])\n"]}],"source":["print(torch.add(x, y))"]},{"cell_type":"markdown","metadata":{"id":"hOLDku89G6O7"},"source":["Addition: providing an output tensor as argument\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1670408916526,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"1Dr7ei6BG6O7","outputId":"07cb5fd9-90a5-4c2f-9b18-1e84c0736774"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 1.4519,  0.7168, -0.4850],\n","        [-0.8998,  2.9129, -0.6419],\n","        [ 1.0427, -1.3704,  1.3449],\n","        [-1.1606,  0.0387,  1.4207],\n","        [-0.2107,  1.4956,  0.9662]])\n"]}],"source":["result = torch.empty(5, 3)\n","torch.add(x, y, out=result)\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"ank04BYEG6O7"},"source":["Addition: in-place\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":250,"status":"ok","timestamp":1670408923337,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"53Exg-LZG6O8","outputId":"173fb039-4e15-4b88-9200-3031ccf6dc4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 1.4519,  0.7168, -0.4850],\n","        [-0.8998,  2.9129, -0.6419],\n","        [ 1.0427, -1.3704,  1.3449],\n","        [-1.1606,  0.0387,  1.4207],\n","        [-0.2107,  1.4956,  0.9662]])\n"]}],"source":["# adds x to y\n","y.add_(x)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"9ADfgr1SG6O8"},"source":["\u003cdiv class=\"alert alert-info\"\u003e\u003ch4\u003eNote\u003c/h4\u003e\u003cp\u003eAny operation that mutates a tensor in-place is post-fixed with an ``_``.\n","    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\u003c/p\u003e\u003c/div\u003e\n","\n","You can use standard NumPy-like indexing with all bells and whistles!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1670408953822,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"qf7grdHBG6O9","outputId":"c25e728c-cb62-421b-ed65-9242a114b447"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 0.2231,  2.2719, -1.7435, -0.0106,  1.1218])\n"]}],"source":["print(x[:, 1])"]},{"cell_type":"markdown","metadata":{"id":"1ocFXMdeG6O9"},"source":["Resizing: If you want to resize/reshape tensor, you can use ``torch.view``:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1670409042552,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"sR0Gr0ZQG6O9","outputId":"7ec68dda-f0a8-4f6e-9ed9-7bbff6587814"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"]}],"source":["x = torch.randn(4, 4)\n","y = x.view(16)\n","z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n","print(x.size(), y.size(), z.size())"]},{"cell_type":"markdown","metadata":{"id":"PYydhziuG6O-"},"source":["If you have a one element tensor, use ``.item()`` to get the value as a\n","Python number\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1670409111473,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"4Ovi9HCjG6PA","outputId":"7817721a-fba6-4b91-b834-2130227a5f52"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-2.2444])\n","-2.2444159984588623\n"]}],"source":["x = torch.randn(1)\n","print(x)\n","print(x.item())"]},{"cell_type":"markdown","metadata":{"id":"FI9rISyCG6PA"},"source":["**Read later:**\n","\n","\n","  100+ Tensor operations, including transposing, indexing, slicing,\n","  mathematical operations, linear algebra, random numbers, etc.,\n","  are described [here](https://pytorch.org/docs/torch).\n","\n","## NumPy Bridge\n","\n","\n","Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n","\n","The Torch Tensor and NumPy array will share their underlying memory\n","locations (if the Torch Tensor is on CPU), and changing one will change\n","the other.\n","\n","### Converting a Torch Tensor to a NumPy Array\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1670409382491,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"G5EUl1VuG6PA","outputId":"594c3f1b-d014-4209-b089-2a7c279941d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1., 1., 1., 1., 1.])\n"]}],"source":["a = torch.ones(5)\n","print(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1670409389074,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"ZWjPKeFKG6PB","outputId":"4b732169-af57-4672-cd47-b31b2720dd29"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1. 1. 1. 1. 1.]\n"]}],"source":["b = a.numpy()\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"VoQCdBIgG6PB"},"source":["See how the numpy array changed in value.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1670409403350,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"AJqNvcsxG6PB","outputId":"7adac36f-216c-4cee-8bb9-482281219be7"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([2., 2., 2., 2., 2.])\n","[2. 2. 2. 2. 2.]\n"]}],"source":["a.add_(1)\n","print(a)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"iix-gDjqG6PC"},"source":["### Converting NumPy Array to Torch Tensor\n","\n","See how changing the np array changed the Torch Tensor automatically"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1670409512201,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"bR4rKk8XG6PD","outputId":"7947086e-ac79-482d-9cfd-595c30172623","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[2. 2. 2. 2. 2.]\n","tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"]}],"source":["import numpy as np\n","a = np.ones(5)\n","b = torch.from_numpy(a)\n","np.add(a, 1, out=a)\n","print(a)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"z1zDQxFuG6PD"},"source":["All the Tensors on the CPU except a CharTensor support converting to\n","NumPy and back.\n","\n","## CUDA Tensors\n","\n","Tensors can be moved onto any device using the ``.to`` method.\n","\n","The default device is 'cpu':"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1670409529787,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"UUDyKHdeG6PE","outputId":"5b788054-b6ca-48be-8b4d-f8c655a5d4fe"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["b.device"]},{"cell_type":"markdown","metadata":{"id":"dIT0cVRlG6PE"},"source":["But if you have a GPU (for example because you run this notebook on [Google Colab](https://colab.research.google.com/)), you can use the 'cuda' device:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhJo-ICZG6PF"},"outputs":[],"source":["# let us run this cell only if CUDA is available\n","# We will use ``torch.device`` objects to move tensors in and out of GPU\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")          # a CUDA device object\n","    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n","    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n","    z = x + y\n","    print(z)\n","    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"]},{"cell_type":"markdown","metadata":{"id":"CC2lU4hvG6PF"},"source":["# Autograd: Automatic Differentiation\n","\n","\n","Central to all neural networks in PyTorch is the ``autograd`` package.\n","Let’s first briefly visit this, and we will then go to training our\n","first model.\n","\n","\n","When executing tensor operations, PyTorch can automatically construct on-the-fly the graph of operations to compute the gradient of any quantity with respect to any tensor involved.\n","\n","Let us see this in more simple terms with some examples.\n","\n","## Tensor\n","\n","``torch.Tensor`` is the central class of the package. If you set its attribute\n","``.requires_grad`` as ``True``, it starts to track all operations on it. When\n","you finish your computation you can call ``.backward()`` and have all the\n","gradients computed automatically. The gradient with respect to a tensor will be\n","accumulated into its ``.grad`` attribute.\n","\n","To stop a tensor from tracking history, you can call ``.detach()`` to detach\n","it from the computation history, and to prevent future computation from being\n","tracked.\n","\n","To prevent tracking history (and using memory), you can also wrap the code block\n","in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n","model because the model may have trainable parameters with\n","``requires_grad=True``, but for which we don't need the gradients\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1670440940767,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"-p2LrSQsG6PG","outputId":"c205d90c-13bf-4319-df0d-a5129e240ac6"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.]])\n"]}],"source":["x = torch.ones(2, 2)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"c9kjqW1MG6PH"},"source":["A Tensor has a Boolean field `requires_grad`, set to `False` by default, which states if PyTorch should build the graph of operations so that gradients with respect to it can be computed."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1670440956929,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"6pUXisduG6PH","outputId":"18462dcb-8c1e-4506-c2d0-2f35f0228d0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n","True\n"]}],"source":["print(x.requires_grad)\n","x.requires_grad_(True)\n","print(x.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"xjyOS5yLG6PH"},"source":["You cannot call `numpy()` on a Tensor that requires grad, you first need to call `detach()`:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1670440961893,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"8ZCP0GiZG6PH","outputId":"13995521-cac3-4b89-940a-db00ae1d1ffb","scrolled":true},"outputs":[{"data":{"text/plain":["array([[1., 1.],\n","       [1., 1.]], dtype=float32)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["x.detach().numpy()"]},{"cell_type":"markdown","metadata":{"id":"oHiPYF5WG6PI"},"source":["Do a tensor operation:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1670440966841,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"yJ6sidwvG6PI","outputId":"a0125803-610a-4753-8fc7-ca0bef8a7c91"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=\u003cAddBackward0\u003e)\n"]}],"source":["y = x + 2\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1670440969745,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"EgYpgvniG6PJ","outputId":"f6d4ee79-ff55-4589-9608-9a56c7c40ead"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["y.requires_grad"]},{"cell_type":"markdown","metadata":{"id":"u-99WdHaG6PK"},"source":["Do more operations:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670440976184,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"zweSV4xGG6PK","outputId":"8209ae6c-671e-4d48-d03c-0fb1397a7f4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=\u003cMulBackward0\u003e)\n","tensor(27., grad_fn=\u003cMeanBackward0\u003e)\n"]}],"source":["z = y * y * 3\n","out = z.mean()\n","\n","print(z) \n","print(out)"]},{"cell_type":"markdown","metadata":{"id":"y4dKNHWVG6PK"},"source":["After the computation is finished, i.e. the 'forward pass', you can call ```.backward()``` and have all the gradients computed automatically."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKS6EoHmG6PL"},"outputs":[],"source":["out.backward()"]},{"cell_type":"markdown","metadata":{"id":"BW7BgRUzG6PL"},"source":["The gradient with respect to a variable is accumulated into the ```.grad``` attribute:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1670441009116,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"6X0Qg5qKG6PL","outputId":"3c833535-e397-47ed-9d4d-9249d12154cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"]}],"source":["print(x.grad)"]},{"cell_type":"markdown","metadata":{"id":"xpVC2Q5aG6PL"},"source":["Let’s call the ``out``\n","*Variable* “$o$”.\n","We have that:\n","\n","$y_i = x_i+2$\n","\n","$z_i = 3 y_i^2$ \n","\n","$o = \\frac{1}{4}\\sum_i z_i$ \n","\n","**Forward pass:**\n","\n","$y_i\\bigr\\rvert_{x_i=1} = 3$\n","\n","$z_i\\bigr\\rvert_{y_i=3} = 27$\n","\n","$o\\bigr\\rvert_{z_i=27} = 27$.\n","\n","Taking derivatives give:\n","\n","$\\frac{d o}{d z_i} = \\frac{1}{4}$\n","\n","$\\frac{d z_i}{d y_i} = 6 y_i$\n","\n","$\\frac{d y_i}{d x_i} =1$\n","\n","\n","hence by the **chain-rule:**\n","\n","$\\frac{d o}{d x_i}\\bigr\\rvert_{x_i=1} = \\frac{d o}{d z_i}\\bigr\\rvert_{z_i=27}\\frac{d z_i}{d y_i}\\bigr\\rvert_{y_i=3}\\frac{d y_i}{d x_i}\\bigr\\rvert_{x_i=1} = \\frac{1}{4} * 18 * 1 = 4.5$."]},{"cell_type":"markdown","metadata":{"id":"t6X6d5FqG6PM"},"source":["Can we do the same for 'y'?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1670441145718,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"toygo_i_G6PM","outputId":"bb2df7f6-53cf-449e-cd30-5e0459181d0f","scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["None\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-12-b81046718426\u003e:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n","  print(y.grad)\n"]}],"source":["print(y.grad)"]},{"cell_type":"markdown","metadata":{"id":"BpNNC_1sG6PM"},"source":["By default, gradients are only retained for leaf variables in the computational graph. Intermediate variables’ gradients are not retained to be inspected later. This was done by design, to save memory. However, you can enforce this behaviour by setting `retain_graph` to `True` when calling `backward`.\n","\n","Let's reset our computations:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVA4JovyG6PN"},"outputs":[],"source":["x = torch.ones(2, 2)\n","x.requires_grad_(True)\n","y = x+2\n","z = 3 * y ** 2 \n","out = z.mean()\n","\n","out.backward(retain_graph=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670441172181,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"_jGyfMDYG6PN","outputId":"09ae8687-9f0a-4c3e-8473-0563cf6ee3d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"]}],"source":["print(x.grad)"]},{"cell_type":"markdown","metadata":{"id":"kIfCly-aG6PP"},"source":["To access the gradient with respect to intermediate (i.e. non-leaf) variables we have to use ```torch.autograd.grad```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1670441181625,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"a5RN2hwmG6PP","outputId":"05fca190-8438-45bc-8958-6ded51bc1c76"},"outputs":[{"data":{"text/plain":["(tensor([[0.2500, 0.2500],\n","         [0.2500, 0.2500]]),)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["torch.autograd.grad(out, z, retain_graph=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206,"status":"ok","timestamp":1670441186033,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"WfjnNh7EG6PR","outputId":"8324b2f3-f43e-4f0d-93a7-e1723263535a","scrolled":true},"outputs":[{"data":{"text/plain":["(tensor([[4.5000, 4.5000],\n","         [4.5000, 4.5000]]),)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["torch.autograd.grad(out, y, retain_graph=True)"]},{"cell_type":"markdown","metadata":{"id":"ixnqXaMPG6PS"},"source":["Let's check the gradient with respect to 'x', and do a few more backward pass:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1670441190821,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"1DEPNr7eG6PS","outputId":"671069ca-ab78-46ec-a1b7-7347a2563256","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"]}],"source":["print(x.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHTk38gwG6PS"},"outputs":[],"source":["out.backward(retain_graph=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":519,"status":"ok","timestamp":1670441198005,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"3vzbjVi8G6PS","outputId":"666c2b86-aae9-4660-e5b2-7c04c0783046"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[9., 9.],\n","        [9., 9.]])\n"]}],"source":["print(x.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1DFsgEpG6PT"},"outputs":[],"source":["out.backward(retain_graph=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1670441202829,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"fgcAgpX4G6PT","outputId":"a7ebaf2c-b553-4f15-a7ce-18eddd7ff2bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[13.5000, 13.5000],\n","        [13.5000, 13.5000]])\n"]}],"source":["print(x.grad)"]},{"cell_type":"markdown","metadata":{"id":"TGucASRcG6PT"},"source":["The gradients must be set to zero manually. Otherwise they will cumulate across several backward calls. \n","This accumulating behavior is desirable in particular to compute the gradient of a sum over multiple examples in a dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1670441217063,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"6ZOuzMOdG6PU","outputId":"38d07432-5139-43cb-cc3a-88e82280ae4d"},"outputs":[{"data":{"text/plain":["tensor([[0., 0.],\n","        [0., 0.]])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["x.grad.data.zero_()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LADUiMUkG6PU"},"outputs":[],"source":["out.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670441221051,"user":{"displayName":"Daniel Huf","userId":"05381614915620709813"},"user_tz":-60},"id":"KSurHHxZG6PU","outputId":"fad69385-74e4-466f-e776-599471d559d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"]}],"source":["print(x.grad)"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}